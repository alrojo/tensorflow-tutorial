{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from tensorflow.python.framework.ops import reset_default_graph\n",
    "\n",
    "def onehot(t, num_classes):\n",
    "    out = np.zeros((t.shape[0], num_classes))\n",
    "    for row, col in enumerate(t):\n",
    "        out[row, col] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle challenge\n",
    "\n",
    "In this lab we will work on a data science challenge from `kaggle.com`.\n",
    "Kaggle is a website to participate in real life challenges.\n",
    "Most competitions on kaggle have a dataset, an accuracy metric and a leaderboard to compare submissions.\n",
    "You can read more about kaggle [here](https://www.kaggle.com/about).\n",
    "\n",
    "OBS: You will need a kaggle account for this exercise!\n",
    "\n",
    "The challenge we will pursue is the [_Leaf Classification_](https://www.kaggle.com/c/leaf-classification) challenge.\n",
    "This is an image recognition challenge where each image is supplemented with three feature vectors (a shape contiguous descriptor, an interior texture histogram, and a ï¬ne-scale margin histogram).\n",
    "\n",
    "The first task in a kaggle competition is to download, understand and preprocess the data, which we will do in the first section.\n",
    "\n",
    "Afterwards, we will look into the type of neural network best suited for handling this type of data (for images, usually convolutional neural networks does a pretty good job).\n",
    "\n",
    "Lastly, we will train the model and put the outputs in a submission file that we can submit to kaggle.\n",
    "Convolution neural networks are one of the most succesfull types of neural networks for image recognition and an integral part of reigniting the interest in neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data\n",
    "\n",
    "Go to the [data section](https://www.kaggle.com/c/leaf-classification/data) of the Leaf Classification competition on kaggle.\n",
    "\n",
    "Next, download all of the available data (`sample_submission.csv`, `train.csv`, `test.csv`, `images`), accept the disclaimer if asked and unzip all folders into the `lab4` folder.\n",
    "Such that\n",
    "\n",
    "```\n",
    ">ls $PATH\\_TO\\_FOLDER/tensorflow_tutorial/lab4\n",
    "images  lab4_Kaggle.ipynb  README.md  sample_submission.csv  test.csv  train.csv\n",
    "```\n",
    "\n",
    "Below we will try to load the data into memory and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_paths = glob.glob(\"images/*\")\n",
    "print \"Amount of images =\", len(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now plot 10 images\n",
    "# as we need all images to have the same dimensionality, we will resize and plot\n",
    "# make the images as small as possible, until the difference between starts to get blurry\n",
    "for i in range(10):\n",
    "    image = imread(image_paths[i], as_grey=True)\n",
    "    #image = resize(image, output_shape=(100, 100))\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(\"name: %s \\n shape:%s\" % (image_paths[i], image.shape))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now loading the train.csv to find features for each training point\n",
    "train = pd.read_csv('train.csv')\n",
    "# notice how we \"only\" have 990 (989+0 elem) images for training, the rest is for testing\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now do similar as in train example above for test.csv\n",
    "test = pd.read_csv('test.csv')\n",
    "# notice that we do not have species here, we need to predict that ..!\n",
    "test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# and now do similar as in train example above for test.csv\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "# accordingly to these IDs we need to provide the probability of a given plant being present\n",
    "sample_submission.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# name all columns in train, should be 3 different columns with 64 values each\n",
    "print train.columns[2::64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try and extract and plot columns\n",
    "X = train.as_matrix(columns=train.columns[2:])\n",
    "print \"X.shape,\", X.shape\n",
    "margin = X[:, :64]\n",
    "shape = X[:, 64:128]\n",
    "texture = X[:, 128:]\n",
    "print \"margin.shape,\", margin.shape\n",
    "print \"shape.shape,\", shape.shape\n",
    "print \"texture.shape,\", texture.shape\n",
    "# let us plot some of the features\n",
    "plt.figure(figsize=(21,7))\n",
    "for i in range(3):\n",
    "    plt.subplot(3,3,1+i*3)\n",
    "    plt.plot(margin[i])\n",
    "    if i == 0:\n",
    "        plt.title('Margin', fontsize=20)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(3,3,2+i*3)\n",
    "    plt.plot(shape[i])\n",
    "    if i == 0:\n",
    "        plt.title('Shape', fontsize=20)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(3,3,3+i*3)\n",
    "    plt.plot(texture[i])\n",
    "    if i == 0:\n",
    "        plt.title('Texture', fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "1. Test various resizings of the image until you have found the smallest resizing of the image where you can still see differentiate between the images.\n",
    "\n",
    "2. From the illustration of the Margin, Shape and Texture, what do you see? And how can it be used to classify?\n",
    "\n",
    "3. Describe what network you would build and how you would represent the data points (image, margin, shape and texture)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Building data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from tensorflow.python.framework.ops import reset_default_graph\n",
    "import os\n",
    "\n",
    "def onehot(t, num_classes):\n",
    "    out = np.zeros((t.shape[0], num_classes))\n",
    "    for row, col in enumerate(t):\n",
    "        out[row, col] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class load_data():\n",
    "    # TODO clean this up with some sub functions\n",
    "    # point is that data_train, data_test and le are public\n",
    "    def __init__(self, train_path, test_path, image_paths, image_shape=(64, 64)):\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "        image_paths = image_paths\n",
    "        image_shape = image_shape\n",
    "        self._load(train_df, test_df, image_paths, image_shape)\n",
    "        \n",
    "    def _load(self, train_df, test_df, image_paths, image_shape):\n",
    "        print \"loading data ...\"\n",
    "        # load train.csv\n",
    "        path_dict = self._path_to_dict(image_paths) # numerate image paths and make it a dict\n",
    "        # merge image paths with data frame\n",
    "        train_image_df = self._merge_image_df(train_df, path_dict)\n",
    "        test_image_df = self._merge_image_df(test_df, path_dict)\n",
    "        # label encoder-decoder (self. because we need it later)\n",
    "        self.le = LabelEncoder().fit(train_image_df['species'])\n",
    "        # labels for train\n",
    "        t_train = self.le.transform(train_image_df['species'])\n",
    "        # getting data\n",
    "        train_data = self._make_dataset(train_image_df, image_shape, t_train)\n",
    "        test_data = self._make_dataset(test_image_df, image_shape)        \n",
    "        # need to reformat the train for validation split reasons in the batch_generator\n",
    "        self.train = self._format_dataset(train_data, for_train=True)\n",
    "        self.test = self._format_dataset(test_data, for_train=False)\n",
    "        print \"data loaded\"\n",
    "        \n",
    "\n",
    "    def _path_to_dict(self, image_paths):\n",
    "        path_dict = dict()\n",
    "        for image_path in image_paths:\n",
    "            num_path = int(os.path.basename(image_path[:-4]))\n",
    "            path_dict[num_path] = image_path\n",
    "        return path_dict\n",
    "\n",
    "    def _merge_image_df(self, df, path_dict):\n",
    "        split_path_dict = dict()\n",
    "        for index, row in df.iterrows():\n",
    "            split_path_dict[row['id']] = path_dict[row['id']]\n",
    "        image_frame = pd.DataFrame(split_path_dict.values(), columns=['image'])\n",
    "        df_image =  pd.concat([image_frame, df], axis=1)\n",
    "        return df_image\n",
    "    \n",
    "\n",
    "    def _make_dataset(self, df, image_shape, t_train=None):\n",
    "        if t_train is not None:\n",
    "            print \"loading train ...\"\n",
    "        else:\n",
    "            print \"loading test ...\"\n",
    "        # make dataset\n",
    "        data = dict()\n",
    "        # merge image with 3x64 features\n",
    "        for i, dat in enumerate(df.iterrows()):\n",
    "            index, row = dat\n",
    "            sample = dict()\n",
    "            if t_train is not None:\n",
    "                features = row.drop(['id', 'species', 'image'], axis=0).values\n",
    "            else:\n",
    "                features = row.drop(['id', 'image'], axis=0).values\n",
    "            sample['margin'] = features[:64]\n",
    "            sample['shape'] = features[64:128]\n",
    "            sample['texture'] = features[128:]\n",
    "            if t_train is not None:\n",
    "                sample['t'] = np.asarray(t_train[i], dtype='int32')\n",
    "            image = imread(row['image'], as_grey=True)\n",
    "            image = resize(image, output_shape=image_shape)\n",
    "            image = np.expand_dims(image, axis=2)\n",
    "            sample['image'] = image   \n",
    "            data[row['id']] = sample\n",
    "            if i % 100 == 0:\n",
    "                print \"\\t%d of %d\" % (i, len(df))\n",
    "        return data\n",
    "\n",
    "    def _format_dataset(self, df, for_train):\n",
    "        # making arrays with all data in, is nessesary when doing validation split\n",
    "        data = dict()\n",
    "        value = df.values()[0]\n",
    "        img_tot_shp = tuple([len(df)] + list(value['image'].shape))\n",
    "        data['images'] = np.zeros(img_tot_shp, dtype='float32')\n",
    "        feature_tot_shp = (len(df), 64)\n",
    "        data['margins'] = np.zeros(feature_tot_shp, dtype='float32')\n",
    "        data['shapes'] = np.zeros(feature_tot_shp, dtype='float32')\n",
    "        data['textures'] = np.zeros(feature_tot_shp, dtype='float32')\n",
    "        if for_train:\n",
    "            data['ts'] = np.zeros((len(df),), dtype='int32')\n",
    "        else:\n",
    "            data['ids'] = np.zeros((len(df),), dtype='int32')\n",
    "        for i, pair in enumerate(df.items()):\n",
    "            key, value = pair\n",
    "            data['images'][i] = value['image']\n",
    "            data['margins'][i] = value['margin']\n",
    "            data['shapes'][i] = value['shape']\n",
    "            data['textures'][i] = value['texture']\n",
    "            if for_train:\n",
    "                data['ts'][i] = value['t']\n",
    "            else:\n",
    "                data['ids'][i] = key\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data ...\n",
      "loading train ...\n",
      "\t0 of 990\n",
      "\t100 of 990\n",
      "\t200 of 990\n",
      "\t300 of 990\n",
      "\t400 of 990\n",
      "\t500 of 990\n",
      "\t600 of 990\n",
      "\t700 of 990\n",
      "\t800 of 990\n",
      "\t900 of 990\n",
      "loading test ...\n",
      "\t0 of 594\n",
      "\t100 of 594\n",
      "\t200 of 594\n",
      "\t300 of 594\n",
      "\t400 of 594\n",
      "\t500 of 594\n",
      "data loaded\n",
      "\n",
      "@@@Shape checking of data sets@@@\n",
      "\n",
      "TRAIN\n",
      "\timages, (990, 64, 64, 1)\n",
      "\tmargins, (990, 64)\n",
      "\tshapes, (990, 64)\n",
      "\ttextures, (990, 64)\n",
      "\tts, (990,)\n",
      "\twhile training, batch_generator will onehot encode ts to (batch_size, num_classes)\n",
      "\n",
      "TEST\n",
      "\timages, (594, 64, 64, 1)\n",
      "\tmargins, (594, 64)\n",
      "\tshapes, (594, 64)\n",
      "\ttextures, (594, 64)\n",
      "\tids, (594,)\n"
     ]
    }
   ],
   "source": [
    "# loading data and setting up constants\n",
    "TRAIN_PATH = \"train.csv\"\n",
    "TEST_PATH = \"test.csv\"\n",
    "IMAGE_PATHS = glob.glob(\"images/*.jpg\")\n",
    "NUM_CLASSES = 99\n",
    "IMAGE_SHAPE = (64, 64, 1)\n",
    "NUM_FEATURES = 64 # for all three features, margin, shape and texture\n",
    "# train holds both X (input) and t (target/truth)\n",
    "data = load_data(train_path=TRAIN_PATH, test_path=TEST_PATH,\n",
    "                 image_paths=IMAGE_PATHS, image_shape=IMAGE_SHAPE[:2])\n",
    "# to visualize the size of the dimensions of the data\n",
    "print\n",
    "print \"@@@Shape checking of data sets@@@\"\n",
    "print\n",
    "print \"TRAIN\"\n",
    "print \"\\timages,\", data.train['images'].shape\n",
    "print \"\\tmargins,\", data.train['margins'].shape\n",
    "print \"\\tshapes,\", data.train['shapes'].shape\n",
    "print \"\\ttextures,\", data.train['textures'].shape\n",
    "print \"\\tts,\", data.train['ts'].shape\n",
    "print \"\\twhile training, batch_generator will onehot encode ts to (batch_size, num_classes)\"\n",
    "print\n",
    "print \"TEST\"\n",
    "print \"\\timages,\", data.test['images'].shape\n",
    "print \"\\tmargins,\", data.test['margins'].shape\n",
    "print \"\\tshapes,\", data.test['shapes'].shape\n",
    "print \"\\ttextures,\", data.test['textures'].shape\n",
    "print \"\\tids,\", data.test['ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batch generator\n",
    "\n",
    "While training, we will not directly access the entire dataset, instead we have a `batch_generator` function to give us inputs aligned with their targets/ids in a size that our model can handle in memory (batch\\_size).\n",
    "\n",
    "Furthermore, the `batch_generator` also handles validation splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class batch_generator():\n",
    "    def __init__(self, data, batch_size=64, num_classes=99,\n",
    "                 num_iterations=5e3, num_features=64, seed=42, val_size=0.1):\n",
    "        print \"initiating batch generator\"\n",
    "        self._train = data.train\n",
    "        self._test = data.test\n",
    "        # get image size\n",
    "        value = self._train['images'][0]\n",
    "        self._image_shape = list(value.shape)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_classes = num_classes\n",
    "        self._num_iterations = num_iterations\n",
    "        self._num_features = num_features\n",
    "        self._seed = seed\n",
    "        self._val_size = 0.1\n",
    "        self._valid_split()\n",
    "        print \"batch generator initiated ...\"\n",
    "\n",
    "    def _valid_split(self):\n",
    "        self._idcs_train, self._idcs_valid = iter(\n",
    "            StratifiedShuffleSplit(self._train['ts'],\n",
    "                                   n_iter=1,\n",
    "                                   test_size=self._val_size,\n",
    "                                   random_state=self._seed)).next()\n",
    "    def _shuffle_train(self):\n",
    "        np.random.shuffle(self._idcs_train)\n",
    "\n",
    "    def _batch_init(self, purpose):\n",
    "        assert purpose in ['train', 'valid', 'test']\n",
    "        batch_holder = dict()\n",
    "        batch_holder['margins'] = np.zeros((self._batch_size, self._num_features), dtype='float32')\n",
    "        batch_holder['shapes'] = np.zeros((self._batch_size, self._num_features), dtype='float32')\n",
    "        batch_holder['textures'] = np.zeros((self._batch_size, self._num_features), dtype='float32')\n",
    "        batch_holder['images'] = np.zeros(tuple([self._batch_size] + self._image_shape), dtype='float32')\n",
    "        if purpose == \"train\":\n",
    "            batch_holder['ts'] = np.zeros((self._batch_size, self._num_classes), dtype='float32')\n",
    "        elif purpose == \"valid\":\n",
    "            batch_holder['ts'] = np.zeros((self._batch_size,), dtype='int32')            \n",
    "        else:\n",
    "            batch_holder['ids'] = np.zeros((self._batch_size,), dtype='int32')\n",
    "        return batch_holder\n",
    "\n",
    "    def gen_valid(self):\n",
    "        batch = self._batch_init(purpose='train')\n",
    "        i = 0\n",
    "        for idx in self._idcs_valid:\n",
    "            batch['margins'][i] = self._train['margins'][idx]\n",
    "            batch['shapes'][i] = self._train['shapes'][idx]\n",
    "            batch['textures'][i] = self._train['textures'][idx]\n",
    "            batch['images'][i] = self._train['images'][idx]\n",
    "            batch['ts'][i] = self._train['ts'][idx]\n",
    "            i += 1\n",
    "            if i >= self._batch_size:\n",
    "                yield batch, i\n",
    "                batch = self._batch_init(purpose='train')\n",
    "                i = 0\n",
    "        if i != 0:\n",
    "            yield batch, i\n",
    "\n",
    "    def gen_test(self):\n",
    "        batch = self._batch_init(purpose='test')\n",
    "        i = 0\n",
    "        for idx in range(len(self._test)):\n",
    "            batch['margins'][i] = self._test['margins'][idx]\n",
    "            batch['shapes'][i] = self._test['shapes'][idx]\n",
    "            batch['textures'][i] = self._test['textures'][idx]\n",
    "            batch['images'][i] = self._test['images'][idx]\n",
    "            batch['ids'][i] = self._test['ids'][idx]\n",
    "            i += 1\n",
    "            if i >= self._batch_size:\n",
    "                yield batch, i\n",
    "                batch = self._batch_init(purpose='test')\n",
    "                i = 0\n",
    "        if i != 0:\n",
    "            yield batch, i\n",
    "            \n",
    "\n",
    "    def gen_train(self):\n",
    "        batch = self._batch_init(purpose='train')\n",
    "        iteration = 0\n",
    "        i = 0\n",
    "        while True:\n",
    "            # shuffling all batches\n",
    "            self._shuffle_train()\n",
    "            for idx in self._idcs_train:\n",
    "                # extract data from dict\n",
    "                batch['margins'][i] = self._train['margins'][idx]\n",
    "                batch['shapes'][i] = self._train['shapes'][idx]\n",
    "                batch['textures'][i] = self._train['textures'][idx]\n",
    "                batch['images'][i] = self._train['images'][idx]\n",
    "                batch['ts'][i] = onehot(np.asarray([self._train['ts'][idx]], dtype='float32'), self._num_classes)\n",
    "                i += 1\n",
    "                if i >= self._batch_size:\n",
    "                    yield batch\n",
    "                    batch = self._batch_init(purpose='train')\n",
    "                    i = 0\n",
    "                    iteration += 1\n",
    "                    if iteration >= self._num_iterations:\n",
    "                        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initiating batch generator\n",
      "batch generator initiated ...\n",
      "\n",
      "@@@Shape checking of batches@@@\n",
      "\n",
      "TRAIN\n",
      "\timages, (64, 64, 64, 1)\n",
      "\tmargins, (64, 64)\n",
      "\tshapes, (64, 64)\n",
      "\ttextures, (64, 64)\n",
      "\tts, (64, 99)\n",
      "\n",
      "VALID\n",
      "\timages, (64, 64, 64, 1)\n",
      "\tmargins, (64, 64)\n",
      "\tshapes, (64, 64)\n",
      "\ttextures, (64, 64)\n",
      "\tts, (64, 99)\n",
      "\n",
      "TEST\n",
      "\timages, (64, 64, 64, 1)\n",
      "\tmargins, (64, 64)\n",
      "\tshapes, (64, 64)\n",
      "\ttextures, (64, 64)\n",
      "\tids, (64,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:18: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "dummy_batch_gen = batch_generator(data, batch_size=64, num_classes=99, num_iterations=5e3, seed=42)\n",
    "train_batch = dummy_batch_gen.gen_train().next()\n",
    "valid_batch, i = dummy_batch_gen.gen_valid().next()\n",
    "test_batch, i = dummy_batch_gen.gen_test().next()\n",
    "\n",
    "print\n",
    "print \"@@@Shape checking of batches@@@\"\n",
    "print\n",
    "print \"TRAIN\"\n",
    "print \"\\timages,\", train_batch['images'].shape\n",
    "print \"\\tmargins,\", train_batch['margins'].shape\n",
    "print \"\\tshapes,\", train_batch['shapes'].shape\n",
    "print \"\\ttextures,\", train_batch['textures'].shape\n",
    "print \"\\tts,\", train_batch['ts'].shape\n",
    "print\n",
    "print \"VALID\"\n",
    "print \"\\timages,\", valid_batch['images'].shape\n",
    "print \"\\tmargins,\", valid_batch['margins'].shape\n",
    "print \"\\tshapes,\", valid_batch['shapes'].shape\n",
    "print \"\\ttextures,\", valid_batch['textures'].shape\n",
    "print \"\\tts,\", valid_batch['ts'].shape\n",
    "print\n",
    "print \"TEST\"\n",
    "print \"\\timages,\", test_batch['images'].shape\n",
    "print \"\\tmargins,\", test_batch['margins'].shape\n",
    "print \"\\tshapes,\", test_batch['shapes'].shape\n",
    "print \"\\ttextures,\", test_batch['textures'].shape\n",
    "print \"\\tids,\", test_batch['ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import fully_connected, convolution2d, flatten, batch_norm, max_pool2d, dropout\n",
    "from tensorflow.python.ops.nn import relu, elu, relu6, sigmoid, tanh, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wrapping convolutions and batch_norm\n",
    "def conv(l_in, num_outputs, kernel_size, scope):\n",
    "    return convolution2d(l_in, num_outputs=num_outputs, kernel_size=kernel_size,\n",
    "                         normalizer_fn=batch_norm, scope=scope)\n",
    "# easy to use pool function\n",
    "def pool(l_in, scope, kernel_size=(3, 3)):\n",
    "    return max_pool2d(l_in, kernel_size=kernel_size, scope=scope) # (3, 3) has shown to work better than (2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define a simple feed forward neural network\n",
    "\n",
    "# hyperameters of the model\n",
    "num_classes = 10\n",
    "height, width, channels = IMAGE_SHAPE\n",
    "# resetting the graph ...\n",
    "reset_default_graph()\n",
    "\n",
    "# Setting up placeholder, this is where your data enters the graph!\n",
    "x_image_pl = tf.placeholder(tf.float32, [None, height, width, channels], name=\"x_image_pl\")\n",
    "x_margin_pl = tf.placeholder(tf.float32, [None, NUM_FEATURES], name=\"x_margin_pl\")\n",
    "x_shape_pl = tf.placeholder(tf.float32, [None, NUM_FEATURES], name=\"x_shape_pl\")\n",
    "x_texture_pl = tf.placeholder(tf.float32, [None, NUM_FEATURES], name=\"x_texture_pl\")\n",
    "is_training_pl = tf.placeholder(tf.bool, name=\"is_training_pl\")\n",
    "\n",
    "# Building the layers of the neural network\n",
    "# we define the variable scope, so we more easily can recognise our variables later\n",
    "\n",
    "## IMAGE\n",
    "#l_conv1_a = conv(x_image_pl, 16, (3, 3), scope=\"l_conv1_a\")\n",
    "#l_conv1_b = conv(l_conv1_a, 16, (3, 3), scope=\"l_conv1_b\")\n",
    "#l_pool1 = pool(l_conv1_b, scope=\"l_pool1\")\n",
    "#l_flatten = flatten(l_pool1, scope=\"flatten\")\n",
    "\n",
    "## FEATURES\n",
    "features = tf.concat(concat_dim=1, values=[x_margin_pl, x_shape_pl, x_texture_pl], name=\"features\")\n",
    "\n",
    "l2 = fully_connected(features, num_outputs=100, activation_fn=relu, scope=\"l2\")\n",
    "l2 = dropout(l2, is_training=is_training_pl, scope=\"l2_dropout\")\n",
    "y = fully_connected(l2, NUM_CLASSES, activation_fn=softmax, scope=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# y_ is a placeholder variable taking on the value of the target batch.\n",
    "ts_pl = tf.placeholder(tf.float32, [None, NUM_CLASSES])\n",
    "\n",
    "def loss_and_acc(preds):\n",
    "    # computing cross entropy per sample\n",
    "    cross_entropy = -tf.reduce_sum(ts_pl * tf.log(preds+1e-10), reduction_indices=[1])\n",
    "    # averaging over samples\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    # if you want regularization\n",
    "    #reg_scale = 0.00001\n",
    "    #regularize = tf.contrib.layers.l2_regularizer(reg_scale)\n",
    "    #params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "    #reg_term = sum([regularize(param) for param in params])\n",
    "    #loss += reg_term\n",
    "    # calculate accuracy\n",
    "    argmax_y = tf.to_int32(tf.argmax(preds, dimension=1))\n",
    "    argmax_t = tf.to_int32(tf.argmax(ts_pl, dimension=1))\n",
    "    correct = tf.to_float(tf.equal(argmax_y, argmax_t))\n",
    "    accuracy = tf.reduce_mean(correct)\n",
    "    return loss, accuracy, argmax_y\n",
    "\n",
    "# loss, accuracy and prediction\n",
    "loss, accuracy, prediction = loss_and_acc(y)\n",
    "\n",
    "# defining our optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.005)\n",
    "\n",
    "# applying the gradients\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y (45, 99)\n"
     ]
    }
   ],
   "source": [
    "#Test the forward pass\n",
    "image_shape = tuple([45]+list(IMAGE_SHAPE))\n",
    "feature_shape = (45, NUM_FEATURES)\n",
    "x_image = np.random.normal(0, 1, image_shape).astype('float32') #dummy data\n",
    "x_margin = np.random.normal(0, 1, feature_shape).astype('float32')\n",
    "x_shape = np.random.normal(0, 1, feature_shape).astype('float32')\n",
    "x_texture = np.random.normal(0, 1, feature_shape).astype('float32')\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "feed_dict = {x_image_pl: x_image,\n",
    "             x_margin_pl: x_margin,\n",
    "             x_shape_pl: x_shape,\n",
    "             x_texture_pl: x_texture,\n",
    "             is_training_pl: False}\n",
    "res = sess.run(fetches=[y], feed_dict=feed_dict)\n",
    "print \"y\", res[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initiating batch generator\n",
      "batch generator initiated ...\n",
      "0, loss: 18773.15589, acc: 0.00000\n",
      "0, loss: 4.59503, acc: 0.00000\n",
      "10, loss: 18779.82873, acc: 0.00000\n",
      "10, loss: 4.55435, acc: 0.06250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:18: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20, loss: 18789.65171, acc: 0.00000\n",
      "20, loss: 4.44546, acc: 0.14062\n",
      "30, loss: 18840.63725, acc: 0.00000\n",
      "30, loss: 4.25002, acc: 0.29688\n",
      "40, loss: 19004.16361, acc: 0.00000\n",
      "40, loss: 3.99012, acc: 0.28125\n",
      "50, loss: 19429.34596, acc: 0.01010\n",
      "50, loss: 3.64508, acc: 0.42188\n",
      "60, loss: 20183.75199, acc: 0.00000\n",
      "60, loss: 3.19794, acc: 0.57812\n",
      "70, loss: 21351.09165, acc: 0.01010\n",
      "70, loss: 2.80895, acc: 0.70312\n",
      "80, loss: 22898.77397, acc: 0.02020\n",
      "80, loss: 2.49366, acc: 0.64062\n",
      "90, loss: 24590.71194, acc: 0.01010\n",
      "90, loss: 2.12914, acc: 0.76562\n",
      "100, loss: 26298.92227, acc: 0.01010\n",
      "100, loss: 1.90989, acc: 0.81250\n",
      "110, loss: 28093.40299, acc: 0.01010\n",
      "110, loss: 2.00117, acc: 0.67188\n",
      "120, loss: 29962.52156, acc: 0.01010\n",
      "120, loss: 1.64753, acc: 0.70312\n",
      "130, loss: 31660.44358, acc: 0.02020\n",
      "130, loss: 1.31476, acc: 0.81250\n",
      "140, loss: 33338.76156, acc: 0.01010\n",
      "140, loss: 1.10151, acc: 0.95312\n",
      "150, loss: 34958.07761, acc: 0.01010\n",
      "150, loss: 1.17183, acc: 0.84375\n",
      "160, loss: 36567.10602, acc: 0.02020\n",
      "160, loss: 0.92753, acc: 0.92188\n",
      "170, loss: 37983.61861, acc: 0.01010\n",
      "170, loss: 0.84920, acc: 0.89062\n",
      "180, loss: 39380.18316, acc: 0.01010\n",
      "180, loss: 0.73554, acc: 0.92188\n",
      "190, loss: 40883.69421, acc: 0.01010\n",
      "190, loss: 0.76090, acc: 0.87500\n",
      "200, loss: 42057.86954, acc: 0.01010\n",
      "200, loss: 0.74065, acc: 0.85938\n",
      "210, loss: 43186.60123, acc: 0.01010\n",
      "210, loss: 0.59901, acc: 0.92188\n",
      "220, loss: 44369.00941, acc: 0.01010\n",
      "220, loss: 0.60271, acc: 0.95312\n",
      "230, loss: 45526.93247, acc: 0.01010\n",
      "230, loss: 0.47276, acc: 0.98438\n",
      "240, loss: 46446.95269, acc: 0.01010\n",
      "240, loss: 0.46891, acc: 0.95312\n",
      "250, loss: 47363.52091, acc: 0.01010\n",
      "250, loss: 0.47628, acc: 0.95312\n",
      "260, loss: 48386.96029, acc: 0.01010\n",
      "260, loss: 0.52037, acc: 0.89062\n",
      "270, loss: 49237.63060, acc: 0.01010\n",
      "270, loss: 0.30508, acc: 0.98438\n",
      "280, loss: 50055.97098, acc: 0.01010\n",
      "280, loss: 0.36444, acc: 0.93750\n",
      "290, loss: 50845.65495, acc: 0.01010\n",
      "290, loss: 0.35206, acc: 0.93750\n",
      "300, loss: 51683.88056, acc: 0.01010\n",
      "300, loss: 0.32652, acc: 1.00000\n",
      "310, loss: 52282.14477, acc: 0.01010\n",
      "310, loss: 0.29690, acc: 0.96875\n",
      "320, loss: 52992.61506, acc: 0.01010\n",
      "320, loss: 0.27948, acc: 0.98438\n",
      "330, loss: 53685.00994, acc: 0.01010\n",
      "330, loss: 0.26998, acc: 0.98438\n",
      "340, loss: 54410.81972, acc: 0.01010\n",
      "340, loss: 0.22425, acc: 1.00000\n",
      "350, loss: 54912.40878, acc: 0.01010\n",
      "350, loss: 0.24075, acc: 0.98438\n",
      "360, loss: 55483.39489, acc: 0.01010\n",
      "360, loss: 0.22434, acc: 0.98438\n",
      "370, loss: 56140.13557, acc: 0.01010\n",
      "370, loss: 0.25797, acc: 0.98438\n",
      "380, loss: 56716.63605, acc: 0.01010\n",
      "380, loss: 0.21855, acc: 1.00000\n",
      "390, loss: 57144.80895, acc: 0.01010\n",
      "390, loss: 0.22289, acc: 0.98438\n",
      "400, loss: 57683.17203, acc: 0.01010\n",
      "400, loss: 0.16886, acc: 1.00000\n",
      "410, loss: 58187.31637, acc: 0.01010\n",
      "410, loss: 0.14564, acc: 1.00000\n",
      "420, loss: 58615.87023, acc: 0.01010\n",
      "420, loss: 0.15745, acc: 1.00000\n",
      "430, loss: 59050.81984, acc: 0.01010\n",
      "430, loss: 0.11225, acc: 1.00000\n",
      "440, loss: 59564.29281, acc: 0.01010\n",
      "440, loss: 0.20970, acc: 0.98438\n",
      "450, loss: 59949.53602, acc: 0.01010\n",
      "450, loss: 0.16450, acc: 1.00000\n",
      "460, loss: 60361.89070, acc: 0.01010\n",
      "460, loss: 0.13516, acc: 1.00000\n",
      "470, loss: 60797.79964, acc: 0.01010\n",
      "470, loss: 0.13954, acc: 1.00000\n",
      "480, loss: 61170.83136, acc: 0.01010\n",
      "480, loss: 0.12934, acc: 1.00000\n",
      "490, loss: 61503.82248, acc: 0.01010\n",
      "490, loss: 0.13315, acc: 1.00000\n",
      "500, loss: 61898.21757, acc: 0.01010\n",
      "500, loss: 0.10089, acc: 1.00000\n",
      "510, loss: 62251.40436, acc: 0.01010\n",
      "510, loss: 0.11222, acc: 1.00000\n",
      "520, loss: 62622.53654, acc: 0.01010\n",
      "520, loss: 0.10968, acc: 0.98438\n",
      "530, loss: 62945.22388, acc: 0.01010\n",
      "530, loss: 0.07110, acc: 1.00000\n",
      "540, loss: 63248.16130, acc: 0.01010\n",
      "540, loss: 0.14622, acc: 0.98438\n",
      "550, loss: 63647.65605, acc: 0.01010\n",
      "550, loss: 0.11263, acc: 1.00000\n",
      "560, loss: 63839.47692, acc: 0.01010\n",
      "560, loss: 0.09465, acc: 1.00000\n",
      "570, loss: 64171.12421, acc: 0.01010\n",
      "570, loss: 0.09937, acc: 0.98438\n",
      "580, loss: 64538.42101, acc: 0.01010\n",
      "580, loss: 0.09668, acc: 1.00000\n",
      "590, loss: 64735.37717, acc: 0.01010\n",
      "590, loss: 0.09336, acc: 1.00000\n",
      "600, loss: 65064.84249, acc: 0.01010\n",
      "600, loss: 0.06742, acc: 1.00000\n",
      "610, loss: 65394.02289, acc: 0.01010\n",
      "610, loss: 0.07420, acc: 1.00000\n",
      "620, loss: 65569.03157, acc: 0.01010\n",
      "620, loss: 0.07692, acc: 1.00000\n",
      "630, loss: 65816.49081, acc: 0.01010\n",
      "630, loss: 0.05065, acc: 1.00000\n",
      "640, loss: 66079.02549, acc: 0.01010\n",
      "640, loss: 0.07105, acc: 1.00000\n",
      "650, loss: 66318.76073, acc: 0.01010\n",
      "650, loss: 0.09144, acc: 1.00000\n",
      "660, loss: 66534.22376, acc: 0.01010\n",
      "660, loss: 0.06706, acc: 1.00000\n",
      "670, loss: 66830.44460, acc: 0.01010\n",
      "670, loss: 0.04996, acc: 1.00000\n",
      "680, loss: 67075.18277, acc: 0.01010\n",
      "680, loss: 0.07995, acc: 1.00000\n",
      "690, loss: 67265.50758, acc: 0.01010\n",
      "690, loss: 0.07490, acc: 1.00000\n",
      "700, loss: 67466.86494, acc: 0.01010\n",
      "700, loss: 0.04922, acc: 1.00000\n",
      "710, loss: 67706.57844, acc: 0.01010\n",
      "710, loss: 0.05185, acc: 1.00000\n",
      "720, loss: 67904.07631, acc: 0.01010\n",
      "720, loss: 0.04807, acc: 1.00000\n",
      "730, loss: 68128.87461, acc: 0.01010\n",
      "730, loss: 0.05645, acc: 1.00000\n",
      "740, loss: 68318.22783, acc: 0.01010\n",
      "740, loss: 0.05177, acc: 1.00000\n",
      "750, loss: 68469.02620, acc: 0.01010\n",
      "750, loss: 0.05282, acc: 1.00000\n",
      "760, loss: 68701.20833, acc: 0.01010\n",
      "760, loss: 0.05337, acc: 1.00000\n",
      "770, loss: 68903.98169, acc: 0.01010\n",
      "770, loss: 0.03952, acc: 1.00000\n",
      "780, loss: 69057.13096, acc: 0.01010\n",
      "780, loss: 0.04147, acc: 1.00000\n",
      "790, loss: 69298.53520, acc: 0.01010\n",
      "790, loss: 0.04795, acc: 1.00000\n",
      "800, loss: 69416.73832, acc: 0.01010\n",
      "800, loss: 0.04287, acc: 1.00000\n",
      "810, loss: 69554.58720, acc: 0.01010\n",
      "810, loss: 0.03591, acc: 1.00000\n",
      "820, loss: 69750.61072, acc: 0.01010\n",
      "820, loss: 0.03311, acc: 1.00000\n",
      "830, loss: 69968.55082, acc: 0.01010\n",
      "830, loss: 0.04372, acc: 1.00000\n",
      "840, loss: 70150.98939, acc: 0.01010\n",
      "840, loss: 0.03278, acc: 1.00000\n",
      "850, loss: 70251.32710, acc: 0.01010\n",
      "850, loss: 0.02668, acc: 1.00000\n",
      "860, loss: 70382.67069, acc: 0.01010\n",
      "860, loss: 0.03316, acc: 1.00000\n",
      "870, loss: 70572.81080, acc: 0.01010\n",
      "870, loss: 0.04646, acc: 1.00000\n",
      "880, loss: 70724.82903, acc: 0.01010\n",
      "880, loss: 0.03394, acc: 1.00000\n",
      "890, loss: 70877.36135, acc: 0.01010\n",
      "890, loss: 0.04680, acc: 1.00000\n",
      "900, loss: 71025.22222, acc: 0.01010\n",
      "900, loss: 0.03050, acc: 1.00000\n",
      "910, loss: 71193.21248, acc: 0.01010\n",
      "910, loss: 0.02215, acc: 1.00000\n",
      "920, loss: 71302.46749, acc: 0.01010\n",
      "920, loss: 0.03212, acc: 1.00000\n",
      "930, loss: 71412.02596, acc: 0.01010\n",
      "930, loss: 0.03144, acc: 1.00000\n",
      "940, loss: 71577.84446, acc: 0.01010\n",
      "940, loss: 0.03580, acc: 1.00000\n",
      "950, loss: 71732.08381, acc: 0.01010\n",
      "950, loss: 0.02102, acc: 1.00000\n",
      "960, loss: 71877.30169, acc: 0.01010\n",
      "960, loss: 0.03047, acc: 1.00000\n",
      "970, loss: 71985.86040, acc: 0.01010\n",
      "970, loss: 0.02680, acc: 1.00000\n",
      "980, loss: 72116.24953, acc: 0.01010\n",
      "980, loss: 0.03237, acc: 1.00000\n",
      "990, loss: 72224.07031, acc: 0.01010\n",
      "990, loss: 0.03269, acc: 1.00000\n",
      "1000, loss: 72339.67235, acc: 0.01010\n",
      "1000, loss: 0.03042, acc: 1.00000\n",
      "1010, loss: 72470.38309, acc: 0.01010\n",
      "1010, loss: 0.02477, acc: 1.00000\n",
      "1020, loss: 72600.52494, acc: 0.01010\n",
      "1020, loss: 0.02265, acc: 1.00000\n",
      "1030, loss: 72706.65440, acc: 0.01010\n",
      "1030, loss: 0.02138, acc: 1.00000\n",
      "1040, loss: 72834.40751, acc: 0.01010\n",
      "1040, loss: 0.02765, acc: 1.00000\n",
      "1050, loss: 72960.91679, acc: 0.01010\n",
      "1050, loss: 0.02365, acc: 1.00000\n",
      "1060, loss: 73066.76551, acc: 0.01010\n",
      "1060, loss: 0.02033, acc: 1.00000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-65b2b6f347e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mcur_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtot_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mfetches_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             feed_dict_valid = {\n",
      "\u001b[0;32m<ipython-input-4-624fcfb9b11a>\u001b[0m in \u001b[0;36mgen_valid\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shapes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shapes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'textures'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'textures'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Training Loop\n",
    "BATCH_SIZE = 64\n",
    "ITERATIONS = 5e3\n",
    "VALIDATION_SIZE = 0.1 # 0.1 is ~ 100 samples for valition\n",
    "SEED = 42\n",
    "\n",
    "batch_gen = batch_generator(data, batch_size=BATCH_SIZE, num_classes=NUM_CLASSES,\n",
    "                            num_iterations=ITERATIONS, seed=SEED, val_size=VALIDATION_SIZE)\n",
    "\n",
    "for i, batch_train in enumerate(batch_gen.gen_train()):\n",
    "    if i>=ITERATIONS:\n",
    "        break\n",
    "    fetches_train = [train_op, loss, accuracy]\n",
    "    feed_dict_train = {\n",
    "        x_image_pl: batch_train['images'],\n",
    "        x_margin_pl: batch_train['margins'],\n",
    "        x_shape_pl: batch_train['shapes'],\n",
    "        x_texture_pl: batch_train['textures'],\n",
    "        ts_pl: batch_train['ts'],\n",
    "        is_training_pl: False\n",
    "    }\n",
    "    res_train = sess.run(fetches=fetches_train, feed_dict=feed_dict_train)\n",
    "    if i % 10 == 0:\n",
    "        cur_acc = 0\n",
    "        cur_loss = 0\n",
    "        tot_num = 0\n",
    "        for batch_valid, num in batch_gen.gen_valid():\n",
    "            fetches_valid = [loss, accuracy]\n",
    "            feed_dict_valid = {\n",
    "                x_image_pl: batch_valid['images'],\n",
    "                x_margin_pl: batch_valid['margins'],\n",
    "                x_shape_pl: batch_valid['shapes'],\n",
    "                x_texture_pl: batch_valid['textures'],\n",
    "                ts_pl: batch_valid['ts'],\n",
    "                is_training_pl: False\n",
    "            }\n",
    "            res_valid = sess.run(fetches=fetches_valid, feed_dict=feed_dict_valid)\n",
    "            cur_loss += res_valid[0]*num\n",
    "            cur_acc += res_valid[1]*num\n",
    "            tot_num += num\n",
    "        valid_loss = cur_loss / float(tot_num)\n",
    "        valid_acc = cur_acc / float(tot_num)\n",
    "        print \"%d, loss: %.5f, acc: %.5f\" % (i, valid_loss, valid_acc)\n",
    "        print \"%d, loss: %.5f, acc: %.5f\" % (i, res_train[1], res_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batch_size = 100\n",
    "num_epochs = 10\n",
    "num_samples_train = x_train.shape[0]\n",
    "num_batches_train = num_samples_train // batch_size\n",
    "num_samples_valid = x_valid.shape[0]\n",
    "num_batches_valid = num_samples_valid // batch_size\n",
    "\n",
    "train_acc, train_loss = [], []\n",
    "valid_acc, valid_loss = [], []\n",
    "test_acc, test_loss = [], []\n",
    "cur_loss = 0\n",
    "loss = []\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        #Forward->Backprob->Update params\n",
    "        cur_loss = 0\n",
    "        for i in range(num_batches_train):\n",
    "            idx = range(i*batch_size, (i+1)*batch_size)\n",
    "            x_batch = x_train[idx]\n",
    "            target_batch = targets_train[idx]\n",
    "            feed_dict_train = {x_pl: x_batch, y_: onehot(target_batch, num_classes), is_training: True}\n",
    "            fetches_train = [train_op, cross_entropy]\n",
    "            res = sess.run(fetches=fetches_train, feed_dict=feed_dict_train)\n",
    "            batch_loss = res[1] #this will do the complete backprob pass\n",
    "            cur_loss += batch_loss\n",
    "        loss += [cur_loss/batch_size]\n",
    "\n",
    "        confusion_valid = ConfusionMatrix(num_classes)\n",
    "        confusion_train = ConfusionMatrix(num_classes)\n",
    "\n",
    "        for i in range(num_batches_train):\n",
    "            idx = range(i*batch_size, (i+1)*batch_size)\n",
    "            x_batch = x_train[idx]\n",
    "            targets_batch = targets_train[idx]\n",
    "            # what to feed our accuracy op\n",
    "            feed_dict_eval_train = {x_pl: x_batch, is_training: False}\n",
    "            # deciding which parts to fetch\n",
    "            fetches_eval_train = [y]\n",
    "            # running the validation\n",
    "            res = sess.run(fetches=fetches_eval_train, feed_dict=feed_dict_eval_train)\n",
    "            # collecting and storing predictions\n",
    "            net_out = res[0] \n",
    "            preds = np.argmax(net_out, axis=-1) \n",
    "            confusion_train.batch_add(targets_batch, preds)\n",
    "\n",
    "        confusion_valid = ConfusionMatrix(num_classes)\n",
    "        for i in range(num_batches_valid):\n",
    "            idx = range(i*batch_size, (i+1)*batch_size)\n",
    "            x_batch = x_valid[idx]\n",
    "            targets_batch = targets_valid[idx]\n",
    "            # what to feed our accuracy op\n",
    "            feed_dict_eval_train = {x_pl: x_batch, is_training: False}\n",
    "            # deciding which parts to fetch\n",
    "            fetches_eval_train = [y]\n",
    "            # running the validation\n",
    "            res = sess.run(fetches=fetches_eval_train, feed_dict=feed_dict_eval_train)\n",
    "            # collecting and storing predictions\n",
    "            net_out = res[0]\n",
    "            preds = np.argmax(net_out, axis=-1) \n",
    "\n",
    "            confusion_valid.batch_add(targets_batch, preds)\n",
    "\n",
    "        train_acc_cur = confusion_train.accuracy()\n",
    "        valid_acc_cur = confusion_valid.accuracy()\n",
    "\n",
    "        train_acc += [train_acc_cur]\n",
    "        valid_acc += [valid_acc_cur]\n",
    "        print \"Epoch %i : Train Loss %e , Train acc %f,  Valid acc %f \" \\\n",
    "        % (epoch+1, loss[-1], train_acc_cur, valid_acc_cur)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "    \n",
    "\n",
    "#get test set score\n",
    "confusion_test = ConfusionMatrix(num_classes)\n",
    "# what to feed our accuracy op\n",
    "feed_dict_eval_train = {x_pl: x_test, is_training: False}\n",
    "# deciding which parts to fetch\n",
    "fetches_eval_train = [y]\n",
    "# running the validation\n",
    "res = sess.run(fetches=fetches_eval_train, feed_dict=feed_dict_eval_train)\n",
    "# collecting and storing predictions\n",
    "net_out = res[0] \n",
    "preds = np.argmax(net_out, axis=-1) \n",
    "confusion_test.batch_add(targets_test, preds)\n",
    "print \"\\nTest set Acc:  %f\" %(confusion_test.accuracy())\n",
    "\n",
    "\n",
    "epoch = np.arange(len(train_acc))\n",
    "plt.figure()\n",
    "plt.plot(epoch,train_acc,'r',epoch,valid_acc,'b')\n",
    "plt.legend(['Train Acc','Val Acc'])\n",
    "plt.xlabel('Epochs'), plt.ylabel('Acc'), plt.ylim([0.75,1.03])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#LOAD the mnist data. To speed up training we'll only work on a subset of the data.\n",
    "#Note that we reshape the data from (nsamples, num_features)= (nsamples, nchannels*rows*cols)  -> (nsamples, nchannels, rows, cols)\n",
    "# in order to retain the spatial arrangements of the pixels\n",
    "data = np.load('mnist.npz')\n",
    "num_classes = 10\n",
    "nchannels,rows,cols = 1,28,28\n",
    "x_train = data['X_train'][:50000].astype('float32')\n",
    "x_train = x_train.reshape((-1,nchannels,rows,cols))\n",
    "targets_train = data['y_train'][:50000].astype('int32')\n",
    "\n",
    "x_valid = data['X_valid'][:500].astype('float32')\n",
    "x_valid = x_valid.reshape((-1,nchannels,rows,cols))\n",
    "targets_valid = data['y_valid'][:500].astype('int32')\n",
    "\n",
    "x_test = data['X_test'][:500].astype('float32')\n",
    "x_test = x_test.reshape((-1,nchannels,rows,cols))\n",
    "targets_test = data['y_test'][:500].astype('int32')\n",
    "\n",
    "print \"Information on dataset\"\n",
    "print \"x_train\", x_train.shape\n",
    "print \"targets_train\", targets_train.shape\n",
    "print \"x_valid\", x_valid.shape\n",
    "print \"targets_valid\", targets_valid.shape\n",
    "print \"x_test\", x_test.shape\n",
    "print \"targets_test\", targets_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot a few MNIST examples\n",
    "idx = 0\n",
    "canvas = np.zeros((28*10, 10*28))\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        canvas[i*28:(i+1)*28, j*28:(j+1)*28] = x_train[idx].reshape((28, 28))\n",
    "        idx += 1\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(canvas, cmap='gray')\n",
    "plt.title('MNIST handwritten digits')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation on contrib layers\n",
    "Check out the [github page](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py) for information on contrib layers (not well documented in their api)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import fully_connected, convolution2d, flatten, batch_norm, max_pool2d, dropout\n",
    "from tensorflow.python.ops.nn import relu, elu, relu6, sigmoid, tanh, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define a simple feed forward neural network\n",
    "\n",
    "# hyperameters of the model\n",
    "num_classes = 10\n",
    "channels = x_train.shape[1]\n",
    "height = x_train.shape[2]\n",
    "width = x_train.shape[3]\n",
    "num_filters_conv1 = 16\n",
    "kernel_size_conv1 = [5, 5] # [height, width]\n",
    "stride_conv1 = [1, 1] # [stride_height, stride_width]\n",
    "num_l1 = 100\n",
    "# resetting the graph ...\n",
    "reset_default_graph()\n",
    "\n",
    "# Setting up placeholder, this is where your data enters the graph!\n",
    "x_pl = tf.placeholder(tf.float32, [None, channels, height, width])\n",
    "l_reshape = tf.transpose(x_pl, [0, 2, 3, 1]) # TensorFlow uses NHWC instead of NCHW\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "# Building the layers of the neural network\n",
    "# we define the variable scope, so we more easily can recognise our variables later\n",
    "l_conv1 = convolution2d(l_reshape, num_filters_conv1, kernel_size_conv1, stride_conv1, scope=\"l_conv1\")\n",
    "l_flatten = flatten(l_conv1, scope=\"flatten\")\n",
    "l1 = fully_connected(l_flatten, num_l1, activation_fn=relu, scope=\"l1\")\n",
    "l1 = dropout(l1, is_training=is_training, scope=\"dropout\")\n",
    "y = fully_connected(l1, num_classes, activation_fn=softmax, scope=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y_ is a placeholder variable taking on the value of the target batch.\n",
    "y_ = tf.placeholder(tf.float32, [None, num_classes])\n",
    "\n",
    "# computing cross entropy per sample\n",
    "cross_entropy = -tf.reduce_sum(y_ * tf.log(y+1e-8), reduction_indices=[1])\n",
    "\n",
    "# averaging over samples\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# defining our optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "\n",
    "# applying the gradients\n",
    "train_op = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Test the forward pass\n",
    "x = np.random.normal(0,1, (45, 1,28,28)).astype('float32') #dummy data\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "res = sess.run(fetches=[y], feed_dict={x_pl: x, is_training: False})\n",
    "print \"y\", res[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Training Loop\n",
    "from confusionmatrix import ConfusionMatrix\n",
    "batch_size = 100\n",
    "num_epochs = 10\n",
    "num_samples_train = x_train.shape[0]\n",
    "num_batches_train = num_samples_train // batch_size\n",
    "num_samples_valid = x_valid.shape[0]\n",
    "num_batches_valid = num_samples_valid // batch_size\n",
    "\n",
    "train_acc, train_loss = [], []\n",
    "valid_acc, valid_loss = [], []\n",
    "test_acc, test_loss = [], []\n",
    "cur_loss = 0\n",
    "loss = []\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        #Forward->Backprob->Update params\n",
    "        cur_loss = 0\n",
    "        for i in range(num_batches_train):\n",
    "            idx = range(i*batch_size, (i+1)*batch_size)\n",
    "            x_batch = x_train[idx]\n",
    "            target_batch = targets_train[idx]\n",
    "            feed_dict_train = {x_pl: x_batch, y_: onehot(target_batch, num_classes), is_training: True}\n",
    "            fetches_train = [train_op, cross_entropy]\n",
    "            res = sess.run(fetches=fetches_train, feed_dict=feed_dict_train)\n",
    "            batch_loss = res[1] #this will do the complete backprob pass\n",
    "            cur_loss += batch_loss\n",
    "        loss += [cur_loss/batch_size]\n",
    "\n",
    "        confusion_valid = ConfusionMatrix(num_classes)\n",
    "        confusion_train = ConfusionMatrix(num_classes)\n",
    "\n",
    "        for i in range(num_batches_train):\n",
    "            idx = range(i*batch_size, (i+1)*batch_size)\n",
    "            x_batch = x_train[idx]\n",
    "            targets_batch = targets_train[idx]\n",
    "            # what to feed our accuracy op\n",
    "            feed_dict_eval_train = {x_pl: x_batch, is_training: False}\n",
    "            # deciding which parts to fetch\n",
    "            fetches_eval_train = [y]\n",
    "            # running the validation\n",
    "            res = sess.run(fetches=fetches_eval_train, feed_dict=feed_dict_eval_train)\n",
    "            # collecting and storing predictions\n",
    "            net_out = res[0] \n",
    "            preds = np.argmax(net_out, axis=-1) \n",
    "            confusion_train.batch_add(targets_batch, preds)\n",
    "\n",
    "        confusion_valid = ConfusionMatrix(num_classes)\n",
    "        for i in range(num_batches_valid):\n",
    "            idx = range(i*batch_size, (i+1)*batch_size)\n",
    "            x_batch = x_valid[idx]\n",
    "            targets_batch = targets_valid[idx]\n",
    "            # what to feed our accuracy op\n",
    "            feed_dict_eval_train = {x_pl: x_batch, is_training: False}\n",
    "            # deciding which parts to fetch\n",
    "            fetches_eval_train = [y]\n",
    "            # running the validation\n",
    "            res = sess.run(fetches=fetches_eval_train, feed_dict=feed_dict_eval_train)\n",
    "            # collecting and storing predictions\n",
    "            net_out = res[0]\n",
    "            preds = np.argmax(net_out, axis=-1) \n",
    "\n",
    "            confusion_valid.batch_add(targets_batch, preds)\n",
    "\n",
    "        train_acc_cur = confusion_train.accuracy()\n",
    "        valid_acc_cur = confusion_valid.accuracy()\n",
    "\n",
    "        train_acc += [train_acc_cur]\n",
    "        valid_acc += [valid_acc_cur]\n",
    "        print \"Epoch %i : Train Loss %e , Train acc %f,  Valid acc %f \" \\\n",
    "        % (epoch+1, loss[-1], train_acc_cur, valid_acc_cur)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "    \n",
    "\n",
    "#get test set score\n",
    "confusion_test = ConfusionMatrix(num_classes)\n",
    "# what to feed our accuracy op\n",
    "feed_dict_eval_train = {x_pl: x_test, is_training: False}\n",
    "# deciding which parts to fetch\n",
    "fetches_eval_train = [y]\n",
    "# running the validation\n",
    "res = sess.run(fetches=fetches_eval_train, feed_dict=feed_dict_eval_train)\n",
    "# collecting and storing predictions\n",
    "net_out = res[0] \n",
    "preds = np.argmax(net_out, axis=-1) \n",
    "confusion_test.batch_add(targets_test, preds)\n",
    "print \"\\nTest set Acc:  %f\" %(confusion_test.accuracy())\n",
    "\n",
    "\n",
    "epoch = np.arange(len(train_acc))\n",
    "plt.figure()\n",
    "plt.plot(epoch,train_acc,'r',epoch,valid_acc,'b')\n",
    "plt.legend(['Train Acc','Val Acc'])\n",
    "plt.xlabel('Epochs'), plt.ylabel('Acc'), plt.ylim([0.75,1.03])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignments 1\n",
    "\n",
    " 1) Note the performance of the standard feedforward neural network. Add a 2D convolution layer before the dense hidden layer and confirm that it increases the generalization performance of the network (try num_filters=16 and filter_size=5 as a starting point). \n",
    " \n",
    " 2) Can the performance be increases even further by stacking more convolution layers ?\n",
    " \n",
    " 3) Maxpooling is a technique for decreasing the spatial resolution of an image while retaining the important features. Effectively this gives a local translational invariance and reduces the computation by a factor of four. In the classification algorithm which is usually desirable. Try to either: \n",
    " \n",
    "     a) add a maxpool layer(add arguement pool_size=2)  after the convolution layer or\n",
    "     b) set add stride=2 to the arguments of the convolution layer. \n",
    "  Verify that this decreases spatial dimension of the image. (print l_conv.output_shape or print   l_maxpool.output_shape). Does this increase the performance of the network (you may need to stack multiple layers or increase the number of filters to increase performance) ?\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of filters\n",
    "Convolution filters can be interpreted as spatial feature detectors picking up different image features such as edges, corners etc. Below we provide code for visualization of the filters. The best results are obtained with fairly large filters of size 9 and either 16 or 36 filters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to start with we print the names of the weights in our graph\n",
    "# to see what operations we are allowed to perform on the variables in our graph, try:\n",
    "#print(dir(tf.all_variables()[0]))\n",
    "# you will notice it has \"name\" and \"value\", which we will build a dictionary from\n",
    "names_and_vars = {var.name: sess.run(var.value()) for var in tf.all_variables()}\n",
    "print(names_and_vars.keys())\n",
    "# getting the name was easy, just use .name on the variable object\n",
    "# getting the value in a numpy array format is slightly more tricky\n",
    "# we need to first get a variable object, then turn it into a tensor with .value()\n",
    "# and the evaluate the tensor with sess.run(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### If you get a key error, then you need to define l_conv1 in your model!\n",
    "np_W = names_and_vars[u'l_conv1/weights:0'] # get the filter values from the first conv layer\n",
    "print np_W.shape, \"i.e. the shape is filter_size, filter_size, num_channels, num_filters\"\n",
    "filter_size, _, num_channels, num_filters = np_W.shape\n",
    "n = int(num_filters**0.5)\n",
    "\n",
    "# reshaping the last dimension to be n by n\n",
    "np_W_res = np_W.reshape(filter_size, filter_size, num_channels, n, n)\n",
    "fig, ax = plt.subplots(n,n)\n",
    "print \"learned filter values\"\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        ax[i,j].imshow(np_W_res[:,:,0,i,j], cmap='gray',interpolation='none')\n",
    "        ax[i,j].xaxis.set_major_formatter(plt.NullFormatter())\n",
    "        ax[i,j].yaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "\n",
    "idx = 1\n",
    "plt.figure()\n",
    "plt.imshow(x_train[idx,0],cmap='gray',interpolation='none')\n",
    "plt.title('Inut Image')\n",
    "plt.show()\n",
    "\n",
    "#visalize the filters convolved with an input image\n",
    "from scipy.signal import convolve2d\n",
    "np_W_res = np_W.reshape(filter_size, filter_size, num_channels, n, n)\n",
    "fig, ax = plt.subplots(n,n,figsize=(9,9))\n",
    "print \"Response from input image convolved with the filters\"\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        ax[i,j].imshow(convolve2d(x_train[1,0],np_W_res[:,:,0,i,j],mode='same'), cmap='gray',interpolation='none')\n",
    "        ax[i,j].xaxis.set_major_formatter(plt.NullFormatter())\n",
    "        ax[i,j].yaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "The visualized filters will likely look most like noise due to the small amount of training data.\n",
    "\n",
    " 1) Try to use 10000 traning examples instead and visualise the filters again\n",
    " \n",
    " 2) Dropout is a very usefull technique for preventing overfitting. Try to add a DropoutLayer after the convolution layer and hidden layer. This should increase both performance and the \"visual appeal\" of the filters\n",
    " \n",
    " 3) Batch normalization is a recent innovation for improving generalization performance. Try to insert batch normalization layers into the network to improve performance. \n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Fun with convolutional networks\n",
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget -N https://s3.amazonaws.com/lasagne/recipes/datasets/mnist_cluttered_60x60_6distortions.npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data the each mnist digit (20x20 pixels) has been placed randomly in a 60x60 canvas. To make the task harder each canvas has then been cluttered with small pieces of digits. In this task it is helpfull for a network if it can focus only on the digit and ignore the rest.\n",
    "\n",
    "The ``TransformerLayer`` lets us do this. The transformer layer learns an affine transformation which lets the network zoom, rotate and skew. If you are interested you should read the paper, but the main idea is that you can let a small convolutional network determine the the parameters of the affine transformation. You then apply the affine transformation to the input data. Usually this also involves downsampling which forces the model to zoom in on the relevant parts of the data. After the affine transformation we can use a larger conv net to do the classification. \n",
    "This is possible because you can backprop through a an affine transformation if you use bilinear interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected, convolution2d, flatten, max_pool2d\n",
    "pool = max_pool2d\n",
    "conv = convolution2d\n",
    "dense = fully_connected\n",
    "from tensorflow.python.ops.nn import relu, softmax\n",
    "from tensorflow.python.framework.ops import reset_default_graph\n",
    "\n",
    "from spatial_transformer import transformer\n",
    "\n",
    "def onehot(t, num_classes):\n",
    "    out = np.zeros((t.shape[0], num_classes))\n",
    "    for row, col in enumerate(t):\n",
    "        out[row, col] = 1\n",
    "    return out\n",
    "\n",
    "NUM_EPOCHS = 500\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.001\n",
    "DIM = 60\n",
    "NUM_CLASSES = 10\n",
    "mnist_cluttered = \"mnist_cluttered_60x60_6distortions.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    data = np.load(mnist_cluttered)\n",
    "    X_train, y_train = data['x_train'], np.argmax(data['y_train'], axis=-1)\n",
    "    X_valid, y_valid = data['x_valid'], np.argmax(data['y_valid'], axis=-1)\n",
    "    X_test, y_test = data['x_test'], np.argmax(data['y_test'], axis=-1)\n",
    "\n",
    "    # reshape for convolutions\n",
    "    X_train = X_train.reshape((X_train.shape[0], 1, DIM, DIM))\n",
    "    X_valid = X_valid.reshape((X_valid.shape[0], 1, DIM, DIM))\n",
    "    X_test = X_test.reshape((X_test.shape[0], 1, DIM, DIM))\n",
    "    \n",
    "    print \"Train samples:\", X_train.shape\n",
    "    print \"Validation samples:\", X_valid.shape\n",
    "    print \"Test samples:\", X_test.shape\n",
    "\n",
    "    return dict(\n",
    "        X_train=np.asarray(X_train, dtype='float32'),\n",
    "        y_train=y_train.astype('int32'),\n",
    "        X_valid=np.asarray(X_valid, dtype='float32'),\n",
    "        y_valid=y_valid.astype('int32'),\n",
    "        X_test=np.asarray(X_test, dtype='float32'),\n",
    "        y_test=y_test.astype('int32'),\n",
    "        num_examples_train=X_train.shape[0],\n",
    "        num_examples_valid=X_valid.shape[0],\n",
    "        num_examples_test=X_test.shape[0],\n",
    "        input_height=X_train.shape[2],\n",
    "        input_width=X_train.shape[3],\n",
    "        output_dim=10,)\n",
    "data = load_data()\n",
    "\n",
    "idx = 0\n",
    "canvas = np.zeros((DIM*10, 10*DIM))\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        canvas[i*DIM:(i+1)*DIM, j*DIM:(j+1)*DIM] = data['X_train'][idx].reshape((DIM, DIM))\n",
    "        idx += 1\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(canvas, cmap='gray')\n",
    "plt.title('Cluttered handwritten digits')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "We use a model where the localization network is a two layer convolution network which operates directly on the image input. The output from the localization network is a 6 dimensional vector specifying the parameters in the affine transformation.\n",
    "\n",
    "We set up the transformer layer to initially do the identity transform, similarly to [1]. If the output from the localization networks is [t1, t2, t3, t4, t5, t6] then t1 and t5 determines zoom, t2 and t4 determines skewness, and t3 and t6 move the center position. By setting the initial values of the bias vector to \n",
    "\n",
    "```\n",
    "|1, 0, 0|\n",
    "|0, 1, 0|\n",
    "```\n",
    "and the final W of the localization network to all zeros we ensure that in the beginning of training the network works as a pooling layer. \n",
    "\n",
    "The output of the localization layer feeds into the transformer layer which applies the transformation to the image input. In our setup the transformer layer downsamples the input by a factor 3.\n",
    "\n",
    "Finally a 2 layer convolution layer and 2 fully connected layers calculates the output probabilities.\n",
    "\n",
    "\n",
    "### The model\n",
    "```\n",
    "Input -> localization_network -> TransformerLayer -> output_network -> predictions\n",
    "   |                                |\n",
    "   >--------------------------------^\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reset_default_graph()\n",
    "def build_model(x_pl, input_width, input_height, output_dim,\n",
    "                batch_size=BATCH_SIZE):\n",
    "    # Setting up placeholder, this is where your data enters the graph!\n",
    "    l_reshape = tf.transpose(x_pl, [0, 2, 3, 1]) # TensorFlow uses NHWC instead of NCHW\n",
    "\n",
    "    # make distributed representation of input image for localization network\n",
    "    loc_l1 = pool(l_reshape, kernel_size=[2, 2], scope=\"loc_l1\")\n",
    "    loc_l2 = conv(loc_l1, num_outputs=8, kernel_size=[5, 5], stride=[1, 1], padding=\"SAME\", scope=\"loc_l2\")\n",
    "    loc_l3 = pool(loc_l2, kernel_size=[2, 2], scope=\"loc_l3\")\n",
    "    loc_l4 = conv(loc_l3, num_outputs=8, kernel_size=[5, 5], stride=[1, 1], padding=\"SAME\", scope=\"loc_l4\")\n",
    "    loc_l4_flatten = flatten(loc_l4, scope=\"loc_l4_flatten\")\n",
    "    loc_l5 = dense(loc_l4_flatten, num_outputs=50, activation_fn=relu, scope=\"loc_l5\")\n",
    "    # set up weights for transformation (notice we always need 6 output neurons)\n",
    "    W_loc_out = tf.get_variable(\"W_loc_out\", [50, 6], initializer=tf.constant_initializer(0.0))\n",
    "    initial = np.array([[1, 0, 0], [0, 1, 0]])\n",
    "    initial = initial.astype('float32')\n",
    "    initial = initial.flatten()\n",
    "    b_loc_out = tf.Variable(initial_value=initial, name='b_loc_out')\n",
    "    loc_out = tf.matmul(loc_l5, W_loc_out) + b_loc_out\n",
    "\n",
    "    # spatial transformer\n",
    "    l_trans1 = transformer(l_reshape, loc_out, out_size=(DIM//3, DIM//3))\n",
    "    l_trans1.set_shape([None, DIM//3, DIM//3, 1])\n",
    "    l_trans1_valid = tf.transpose(l_trans1, [0, 2, 3, 1]) # Back into NCHW for validation\n",
    "\n",
    "    print \"Transformer network output shape: \", l_trans1.get_shape()\n",
    "\n",
    "    # classification network\n",
    "    class_l1 = conv(l_trans1, num_outputs=16, kernel_size=[3, 3], scope=\"class_l1\")\n",
    "    class_l2 = pool(class_l1, kernel_size=[2, 2], scope=\"class_l2\")\n",
    "    class_l3 = conv(class_l2, num_outputs=16, kernel_size=[3, 3], scope=\"class_l3\")\n",
    "    class_l4 = pool(class_l3, kernel_size=[2, 2], scope=\"class_l4\")\n",
    "    class_l4_flatten = flatten(class_l4, scope=\"class_l4_flatten\")\n",
    "    class_l5 = dense(class_l4_flatten, num_outputs=256, activation_fn=relu, scope=\"class_l5\")\n",
    "    l_out = dense(class_l5, num_outputs=output_dim, activation_fn=softmax, scope=\"l_out\")\n",
    "\n",
    "    return l_out, l_trans1_valid\n",
    "\n",
    "x_pl = tf.placeholder(tf.float32, [None, 1, DIM, DIM])\n",
    "model, l_transform = build_model(x_pl, DIM, DIM, NUM_CLASSES)\n",
    "#model_params = lasagne.layers.get_all_params(model, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# y_ is a placeholder variable taking on the value of the target batch.\n",
    "y_pl = tf.placeholder(tf.float32, shape=[None, NUM_CLASSES])\n",
    "lr_pl = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "# computing cross entropy per sample\n",
    "cross_entropy = -tf.reduce_sum(y_pl * tf.log(model+1e-8), reduction_indices=[1])\n",
    "\n",
    "# averaging over samples\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# defining our optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr_pl)\n",
    "\n",
    "# applying the gradients\n",
    "train_op = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Test the forward pass\n",
    "x = np.random.normal(0,1, (45, 1,60,60)).astype('float32') #dummy data\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "res = sess.run(fetches=[model], feed_dict={x_pl: x})\n",
    "print \"y\", res[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "Unfortunately NVIDIA has yet to squeeze a TitanX into a labtop and training convnets on CPU is painfully slow. After 10 epochs you should see that model starts to zoom in on the digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_epoch(X, y, learning_rate):\n",
    "    num_samples = X.shape[0]\n",
    "    num_batches = int(np.ceil(num_samples / float(BATCH_SIZE)))\n",
    "    costs = []\n",
    "    correct = 0\n",
    "    for i in range(num_batches):\n",
    "        if i % 10 == 0:\n",
    "            print i,\n",
    "        idx = range(i*BATCH_SIZE, np.minimum((i+1)*BATCH_SIZE, num_samples))\n",
    "        X_batch_tr = X[idx]\n",
    "        y_batch_tr = y[idx]\n",
    "        fetches_tr = [train_op, cross_entropy, model]\n",
    "        feed_dict_tr = {x_pl: X_batch_tr, y_pl: onehot(y_batch_tr, NUM_CLASSES), lr_pl: learning_rate}\n",
    "        res = sess.run(fetches=fetches_tr, feed_dict=feed_dict_tr)\n",
    "        cost_batch, output_train = tuple(res[1:3])\n",
    "        costs += [cost_batch]\n",
    "        preds = np.argmax(output_train, axis=-1)\n",
    "        correct += np.sum(y_batch_tr == preds)\n",
    "    print \"\"\n",
    "    return np.mean(costs), correct / float(num_samples)\n",
    "\n",
    "\n",
    "def eval_epoch(X, y):\n",
    "    fetches_val = [model, l_transform]\n",
    "    feed_dict_val = {x_pl: X}\n",
    "    res = sess.run(fetches=fetches_val, feed_dict=feed_dict_val)\n",
    "    output_eval, transform_eval = tuple(res)\n",
    "    preds = np.argmax(output_eval, axis=-1)\n",
    "    acc = np.mean(preds == y)\n",
    "    return acc, transform_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_accs, train_accs, test_accs = [], [], []\n",
    "learning_rate=0.0001\n",
    "try:\n",
    "    for n in range(NUM_EPOCHS):\n",
    "        train_cost, train_acc = train_epoch(data['X_train'], data['y_train'], learning_rate)\n",
    "        valid_acc, valid_trainsform = eval_epoch(data['X_valid'], data['y_valid'])\n",
    "        test_acc, test_transform = eval_epoch(data['X_test'], data['y_test'])\n",
    "        valid_accs += [valid_acc]\n",
    "        test_accs += [test_acc]\n",
    "        train_accs += [train_acc]\n",
    "\n",
    "        # learning rate annealing\n",
    "        if (n+1) % 10 == 0:\n",
    "            learning_rate = learning_rate * 0.7\n",
    "            print \"New LR:\", learning_rate\n",
    "\n",
    "        print \"Epoch {0}: Train cost {1}, Train acc {2}, val acc {3}, test acc {4}\".format(\n",
    "                n, train_cost, train_acc, valid_acc, test_acc)\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot errors and zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "plt.plot(1-np.array(train_accs), label='Training Error')\n",
    "plt.plot(1-np.array(valid_accs), label='Validation Error')\n",
    "plt.legend(fontsize=20)\n",
    "plt.xlabel('Epoch', fontsize=20)\n",
    "plt.ylabel('Error', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,14))\n",
    "for i in range(3):\n",
    "    plt.subplot(321+i*2)\n",
    "    plt.imshow(data['X_test'][i].reshape(DIM, DIM), cmap='gray', interpolation='none')\n",
    "    if i == 0:\n",
    "        plt.title('Original 60x60', fontsize=20)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(322+i*2)\n",
    "    plt.imshow(test_transform[i].reshape(DIM//3, DIM//3).T, cmap='gray', interpolation='none')\n",
    "    if i == 0:\n",
    "        plt.title('Transformed 20x20', fontsize=20)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# A few pointers for image classification\n",
    "If you want do image classification using a pretrained model is often a good choice, especially if you have limited amounts of labeled data. \n",
    "\n",
    "An often used pretrained network is the VGG16 and VGG19. Lasagne has pretrained models in the [modelzoo](https://github.com/Lasagne/Recipes/tree/master/modelzoo). Torch7 and Tensorflow have similar pretrained models that you can find with google. \n",
    "\n",
    "Currently the best performing image networks is [ResNet](https://arxiv.org/pdf/1512.03385v1.pdf). Torch7 has an interesting blog post about Residual nets. http://torch.ch/blog/2016/02/04/resnets.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
